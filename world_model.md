The paper "Autoencoding beyond pixels using a learned similarity metric" introduced a method which combines a variational autoencoder with a generative adversarial network, so that we could  train our GAN discriminator(using image data) in a feature representation perspective instead of a pixel-wise perspective which could causes large error whereas a human would barely notice the change. The original VAE is consisting of an encoder and a decoder, and a GAN is consisting of a generator and a discriminator. In this paper, the author combines them together and create a VAE/GAN model has a form of a encoder connected to a decoder which replaced the generator of GAN and then connected to the discriminator. VAE/GAN is an unsupervised generative model and learns to encode, generate and compare dataset samples simultaneously. The advantage of this approach is that it produces better image sample compare to models trained with element-wise error measures, since it is trained with a more abstract, higher level and meaningful feature-wise metric and the training results in a image representation with distangled factors of variation. Since the traditional element-wise reconstruction errors and other signals with invariances are not enough for images, the author replaced the VAE reconstruction error with reconstruction error of GAN discriminator, since GAN learned rich similarity metric for images to discriminate them from "non-images" during the training. For the details of VAE, the encoding part which convert a data sample x into latent representation and decoding part which convert the latent representation back to data space are as follow: $z \sim { Encode}(x) = q(z|x),  \tilde{x} \sim Dec(z)=p(x|z)$. The regularization part is by using a prior over p(z) and $z \sim \N(O,I)$ is used. The loss function is summation of negative expected log-likelihood plus the prior which is $Loss^{pixel}_{like}+Loss_{prior}$ respectively: $Loss^{pixel}_{like} = -E_{q(z|x)}[log\frac{p(x|z)p(z)}{q(z|x)}], Loss_{prior}=D_{KL}(q(z|x)||p(z))$. D_{KL} is Kullback Leibler divergence. For the GAN, the generator maps latent representation from VAE to data space and discriminator assign a probability $y=Dis(x)\in [0,1]$, x is the actual data sample and for 1-y, x is the data generated by Gen(z). The loss function is expected to minimize the cross entropy and designed as: $Loss_{GAN}=log(Dis(x)+log(1-Dis(Gen(z))))$. But all the equation above is used in traditional model and in the VAE/GAN model we combined two models together which make the element-wise reconstruction errors no enough to train the model, hence the author introduced a Gaussian observation model for $Dis_l(x)$ with mean $Dis_l(\tilde{x})$: $p(Dis_l(x)|z)=N(Dis_l(x)|Dis(\tilde{x}),I)$ .Where $l$ is the layer number and $\tilde{x} \sim Dec(z)$ is the sample from the decoder of x. So finally we are going to replace the VAE(pixel-wise) error with  $Loss^{Dis_l}_{like} = -E_{q(z|x)}[log (p(Dis_l(x)|z))], Loss_{prior}=D_{KL}(q(z|x)||p(z))$ and train the model with the combined method $Loss=loss_{prior}+loss_{like}^{Dis_l}+loss_{GAN}$. The author also observed some problems during the development and tried to make three adjustment: 1. Limiting error signals to relevant networks, since they found that they would get better result by not backpropagating the error signal from $loss_{GAN}$ to Enc. 2. Weighting VAE v.s. GAN, they introduced a parameter $\gamma$ on $loss_{like}^{Dis_l}$ instead of applying it on entire model. 3. Discriminating based on samples from P(z) and q(z|x). For this project, we are going to mimic the way how author combines AVE and GAN  together in the world models, and take advantage of practice adjustments given by author according to our situation.
